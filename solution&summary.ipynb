{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avito Challenge: Top5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.About the competiton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is my second competition on kaggle and I finally reach my first solo silver medal(88/1917), which make me really excited and it's impressive. Actually I spent about 40 days on this competition. It shows that if you spend enough time on kaggle, you will finally achieve a good result. Although solo is a very tough way to go, I still feel enthusiastic about this competition. There are tabular features, image features and text features. Therefore, we have a lot of things to do and if I have more time, I believe I can improve my score by including more useful features. Besides, some skills about feature engineering that I learnt from Talkingdata's competition also help me a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the data in this competition, I think it's such a great competition. Because it has tabular data, text data and image data. What's more, it also has some supplyment data that gives us new ideas to try. It seems that competitors have lots of things to do. For novice like me, we have lots of knowledge to trial and error. I really learnt a lot in this competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.My solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, I use 5 fold cv. When I did feature seclection, it always kept a constant gap between public LB and my cv scores(about 0.004x). With the help of GCP and small dataset, I could use 5 fold cv, even 10 fold cv. Therefore, I also use 5 fold cv to do stacking and I found that averaging 5 fold results behave better than using the whole dataset. No wonder so many competitor like to use 5 or 10 fold cv. Besides, I also averaged 10 fold cv results near the end of the competition to see how much it can improve my model. Finally, it only improved about 0.0001. Not bad!\n",
    "\n",
    "What's more, I also felt a little confused that near the end of the competition, I have bulit lots of useful features and had to use diversitied models to do stacking. However, it took much time to make OOF(out_of_fold) and I hope that I can figure out a good solution next time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featured engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some useful features I have added to my final single model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Groupby features__:\n",
    "According to lesson learnt from Talkingdata's competition, I have tried to make a lot of Grouby features, including train_acitve and test_active data. I make some notes in this part. For more details, you can refer to it. Finally, I chose about 10 features according to local cv and public LB and it improved a lot in my model(about 0.0003~0.0005).\n",
    "Besides, I also found some more beautiful Groupby ideas from others' solution after the competition. For example,  \"df['price']-groupby['price'].mean\", it actually includes some practical information in it. Maybe next time, I will build some more different Groupby features about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Period features__:\n",
    "In this part, I mainly refer to this useful [kernel](https://www.kaggle.com/bminixhofer/aggregated-features-lightgbm). It offered three features extracted from the period files, which improved my model(about 0.0003~0.0005). Besides, I also tried to extract more features from the period files. For example, I fill the missing data from the kernel above and group by other categorical features while combining data from period files and training files. However, these methods did not work for me. But I still believe that there are some smart ideas to create new features from period files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Image features__:\n",
    "There are lots of image in the data, which means that it will take too much time to extract meta image features from them. I am not familiar with knowledge about multithreading and multiprocssing. I did not extract these features by myself, since it is not worthwhile for me to do that. But luckily, I found some csv files about image features that have been extracted on github. Finally, those meta image features helped me decline my public LB(about 0.0003~0.0004)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Text features(Tf-idf)__:\n",
    "About text features, I used TF-IDF according to baseline kernel. This is a new method I learnt in this competition and it's an efficient way to make new features from text information. Actually, I realised TF-IDF features were very important near the end of the competition. It had lots of parameter to tun, such as max features and ngram. Also, I declined the number of max features and it gave me better public LB scores, which made me very confused. Therefore, it is worthwhile to do more research on this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __weak model from other kernel__:\n",
    "Sometimes, some kernels' new features did not improve my public LB. But they bulit a different model to train the data. I think these kernel could also be useful by creating OOF for them. It's the most important trick I learnt from this competition. Therefore, I created new OOF for these kernel and used them as new features in my model. Then, I forked kernels, including [RNN model](https://www.kaggle.com/shanth84/rnn-detailed-explanation-0-2246), [CNN for title and description](https://www.kaggle.com/jingqliu/stacked-model-cnn-xgboost), [Catboost for simple features](https://www.kaggle.com/nicapotato/simple-catboost), [Simple ridge model](https://www.kaggle.com/yekenot/ridge-text-cat). Finally, it gave me a big boost on public LB(about 0.0009). Stacking is important, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some features that I have not tried or tried but did not work:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Features from missing data__:\n",
    "During the competition, I also read some kernel about filling missing data, such as this [kernel](https://www.kaggle.com/christofhenkel/text2image-top-1). So I also tried to build a simple RandomForest model to predict missing value in 'price' and 'image_top_1' and I also included data from train_active and test_active. But it did not help me a lot on public LB. Besides, I found on [github](https://github.com/arroqc/Avito-Kaggle) that some people created different NN model to predict 'price' and 'image_top_1' and then used them as new features. It is an impressive way and I learnt a lot from it. However, due to limited knowledge about NN structure, I did not try this idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __SVD component from TF-IDF__:\n",
    "Due to limited time of the competition, I did not use truncateSVD to get features columns from TF-IDF sparse matrix. Actually, I can combine SVD component and TF-IDF sparse matrix to train. I guess it may help improve my public LB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __Target encoding__:\n",
    "Fine, still TE! Still no improvement! I added noise and smoothing method for TE code according to others' code. What made me confused was that, it not always worked. So I did not add them to my final model. Maybe there were some necessary keys while doing TE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __pseudo-labelling__:\n",
    "Because of lesson learnt in the Talkingdata competition and extra active data in this competition, I wanted to try pseudo-labelling using train_active and test_active data. But as the [discussion](https://www.kaggle.com/c/avito-demand-prediction/discussion/56816) said, pseudo-labelling cannot benefit a lot in this competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* __VCG16 features__:\n",
    "I did not learn RNN model. So I always try to look through Kernel and Discussion to find useful information. In this competition, I found a [kernel](https://www.kaggle.com/bguberfain/vgg16-train-features) about RNN that might help me a lot. It had extracted VCG16 features for us and I used them in my model. However, it just gave me a 0.0001 boost on public LB. What's more, when I used bigger num_leaves, VCG16 features did not give me a boost. Then I gave up them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only remember these feature engineering ideas. For more skills, please view other people's top solutions on the [discussion](https://www.kaggle.com/c/avito-demand-prediction/discussion). Those top solutions always impress me a lot after the competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this competition, I did not spend too much time on tuning the parameter. From my experience, lower learning rate and more num_leaves can benefit the model. Finally, 0.02 learning rate and 1000 num_leaves is limit I can reach. Besides, I still gave up Bayesian Optimization because 5 fold cv took lots of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before model ensembling, my best single model is LightGBM with 10 fold cv(about 0.2198 on public LB). I mainly used xgboost, linear xgboost (booster=gblinear), lightgbm, poisson lightgbm(objective=poisson), ridge for TF-IDF features, bm25 from github for 1st level stacking. After 1st level stacking, my model improved(about 0.001). So stacking is a helpful tool in this competition. What's more, I have noticed that different features with different models usually end up with a bigger boost on my stacking framework. Due to limited time, I did not try more feature engineering ideas. Maybe adding more diversified features and models can help improve my public LB score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
