{
  "cells": [
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a9fd0fb7c206ec1aea8d684787686a0da8e0b6a1",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Initially forked from Bojan's kernel here: https://www.kaggle.com/tunguz/bow-meta-text-and-dense-features-lb-0-2242/code\n#improvement using kernel from Nick Brook's kernel here: https://www.kaggle.com/nicapotato/bow-meta-text-and-dense-features-lgbm\n#Used oof method from Faron's kernel here: https://www.kaggle.com/mmueller/stacking-starter?scriptVersionId=390867\n#Used some text cleaning method from Muhammad Alfiansyah's kernel here: https://www.kaggle.com/muhammadalfiansyah/push-the-lgbm-v19\nimport time\nnotebookstart= time.time()\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport gc\nprint(\"Data:\\n\",os.listdir(\"../input\"))\nfrom tqdm import tqdm\n\n# Models Packages\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n# Gradient Boosting\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.cross_validation import KFold\n\n# Tf-Idf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom nltk.corpus import stopwords \n\n# Viz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport string\n\nNFOLDS = 5\nSEED = 42\nVALID = True\n\nclass SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n        if(seed_bool == True):\n            params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n        \ndef get_oof(clf, x_train, y, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        print('\\nFold {}'.format(i))\n        x_tr = x_train[train_index]\n        y_tr = y[train_index]\n        x_te = x_train[test_index]\n        y_val_kf = y[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        val_pred = clf.predict(x_te)\n        oof_train[test_index] = val_pred\n        oof_test_skf[i, :] = clf.predict(x_test)\n        \n        cv_rms = sqrt(mean_squared_error(y_val_kf, val_pred))\n        print('fold cv {} AUC score is {:.6f}'.format(i, cv_rms))\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n    \ndef rmse(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power((y - y0), 2)))\n\nprint(\"\\nData Load Stage\")\ndebug= 0\nif debug:\n    training = pd.read_csv('../input/avito-demand-prediction/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"], nrows=50000)\n    testing = pd.read_csv('../input/avito-demand-prediction/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"], nrows=20000)\nelse:\n    training = pd.read_csv('../input/avito-demand-prediction/train.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\n    testing = pd.read_csv('../input/avito-demand-prediction/test.csv', index_col = \"item_id\", parse_dates = [\"activation_date\"])\ntraindex = training.index\ntestdex = testing.index\n\nntrain = training.shape[0]\nntest = testing.shape[0]\nkf = KFold(ntrain, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n\ny = training.deal_probability.copy()\nprint('Train shape: {} Rows, {} Columns'.format(*training.shape))\nprint('Test shape: {} Rows, {} Columns'.format(*testing.shape))\n\nprint(\"Combine Train and Test\")\ndf = pd.concat([training,testing],axis=0)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "dfd83b69a4bac2a0b5179edb85726573f023f4d6"
      },
      "cell_type": "code",
      "source": "idx=list(df.index)\ndf.drop(\"deal_probability\",axis=1, inplace=True)\ndel testing\ngc.collect()\nprint('\\nAll Data shape: {} Rows, {} Columns'.format(*df.shape))\n\nprint(\"Combine new features\")\nperiod_features = pd.read_csv('../input/aggregated-features-lightgbm/aggregated_features.csv')\nagg_features = pd.read_csv('../input/active-csv-categorical-features-combining0/agg_final.csv',index_col = \"item_id\")\n#agg2 = pd.read_csv('../input/missing-value-aggregated-features2/lgsub.csv',index_col = \"item_id\")\nrnn=pd.read_csv('../input/rnn-pred/rnn_pred.csv',usecols=['item_id','rnn_pred'],index_col='item_id')\ndfdex=df.index\ndf = df.merge(period_features, how=\"left\", on='user_id').set_index(dfdex)\ndf = df.merge(agg_features, left_index = True, right_index = True)\n#df = df.merge(agg2, left_index = True, right_index = True)\ndf = df.merge(rnn, left_index = True, right_index = True)\ndf.drop(\"user_id\",axis=1, inplace=True)\n\nprint(\"Feature Engineering\")\ndf[\"price\"] = np.log(df[\"price\"]+0.001)\ndf[\"price\"].fillna(df.price.mean(),inplace=True)\ndf[\"image_top_1\"].fillna(-999,inplace=True)\ndf['avg_days_up_user'].fillna(-999,inplace=True)\ndf['avg_times_up_user'].fillna(-999,inplace=True)\n\nprint(\"\\nCreate Time Variables\")\ndf[\"Weekday\"] = df['activation_date'].dt.weekday\ndf[\"Weekd of Year\"] = df['activation_date'].dt.week\ndf[\"Day of Month\"] = df['activation_date'].dt.day\n\n#I add it--2\nprint(\"aggregate features...\")\nagg_cols = ['region', 'city', 'image_top_1', \"param_1\", \"param_2\", \"param_3\"]\nfor c in tqdm(agg_cols):\n    gp = training.groupby(c)['deal_probability']\n    mean = gp.mean()\n    std  = gp.std()\n    df[c + '_deal_probability_avg'] = df[c].map(mean)\n    df[c + '_deal_probability_std'] = df[c].map(std)\n    \ngp = training.groupby('image_top_1')['price']\nmean = gp.mean()\ncount = gp.size()\ndf['image_top_1' + '_price_avg'] = df['image_top_1'].map(mean)\ndf['image_top_1' + 'count'] = df['image_top_1'].map(count)\ndel training, gp, mean, count\ngc.collect()\n\n# Create Validation Index and Remove Dead Variables\ntraining_index = df.loc[df.activation_date<=pd.to_datetime('2017-04-07')].index\nvalidation_index = df.loc[df.activation_date>=pd.to_datetime('2017-04-08')].index\ndf.drop([\"activation_date\",\"image\"],axis=1,inplace=True)\n\nprint(\"\\nEncode Variables\")\ncategorical = [\"region\",\"city\",\"parent_category_name\",\"category_name\",\"user_type\",\"image_top_1\",\"param_1\",\"param_2\",\"param_3\"]\nprint(\"Encoding :\",categorical)\n\n# Encoder:\nlbl = preprocessing.LabelEncoder()\nfor col in categorical:\n    df[col].fillna('Unknown')\n    df[col] = lbl.fit_transform(df[col].astype(str))\n    \nprint(\"\\nText Features\")\n\n# Feature Engineering \n\n# Meta Text Features\ntextfeats = [\"description\", \"title\"]\ndf['desc_punc'] = df['description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\nfor cols in textfeats:\n    df[cols] = df[cols].astype(str) \n    df[cols] = df[cols].astype(str).fillna('missing') # FILL NA\n    df[cols] = df[cols].str.lower() # Lowercase all text, so that capitalized words dont get treated differently\n    df[cols + '_num_words'] = df[cols].apply(lambda comment: len(comment.split())) # Count number of Words\n    df[cols + '_num_unique_words'] = df[cols].apply(lambda comment: len(set(w for w in comment.split())))\n    df[cols + '_words_vs_unique'] = df[cols+'_num_unique_words'] / df[cols+'_num_words'] * 100 # Count Unique Words\n    \n\nprint(\"\\n[TF-IDF] Term Frequency Inverse Document Frequency Stage\")\nrussian_stop = set(stopwords.words('russian'))\n\ntfidf_para = {\n    \"stop_words\": russian_stop,\n    \"analyzer\": 'word',\n    \"token_pattern\": r'\\w{1,}',\n    \"sublinear_tf\": True,\n    \"dtype\": np.float32,\n    \"norm\": 'l2',\n    #\"min_df\":5,\n    #\"max_df\":.9,\n    \"smooth_idf\":False\n}\n\n\ndef get_col(col_name): return lambda x: x[col_name]\n##I added to the max_features of the description. It did not change my score much but it may be worth investigating\nvectorizer = FeatureUnion([\n        ('description',TfidfVectorizer(\n            ngram_range=(1, 2),\n            max_features=17000,\n            **tfidf_para,\n            preprocessor=get_col('description'))),\n        ('title',CountVectorizer(\n            ngram_range=(1, 2),\n            stop_words = russian_stop,\n            min_df=2,\n            #max_features=7000,\n            preprocessor=get_col('title')))\n    ])\n    \nstart_vect=time.time()\n#Fit my vectorizer on the entire dataset instead of the training rows\n#Score improved by .0001\nvectorizer.fit(df.to_dict('records'))\nready_df = vectorizer.transform(df.to_dict('records'))\ntfvocab = vectorizer.get_feature_names()\nprint(\"Vectorization Runtime: %0.2f Minutes\"%((time.time() - start_vect)/60))\n\n# Drop Text Cols\ndf.drop(textfeats, axis=1,inplace=True)\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nridge_params = {'alpha':20.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n\n#Ridge oof method from Faron's kernel\n#I was using this to analyze my vectorization, but figured it would be interesting to add the results back into the dataset\n#It doesn't really add much to the score, but it does help lightgbm converge faster\nridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\nridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:ntrain], y, ready_df[ntrain:])\n\nrms = sqrt(mean_squared_error(y, ridge_oof_train))\nprint('Ridge OOF RMSE: {}'.format(rms))\n\nprint(\"Modeling Stage\")\n\nridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n\ndf['ridge_preds'] = ridge_preds\n\nsub = pd.DataFrame(ridge_preds,columns=['rid_pred'])\n#sub1 = pd.DataFrame(idx,columns=['item_id'])\nsub['item_id']=idx\nsub.to_csv('final_ridge.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}